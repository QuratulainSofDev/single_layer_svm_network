{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class SingleLayerNeuralNetwork:\n",
        "    def __init__(self, input_size, num_classes, learning_rate=0.15, num_iterations=1000):\n",
        "        self.input_size = input_size\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W = np.random.randn(input_size, num_classes)\n",
        "        self.b = np.zeros((1, num_classes))\n",
        "\n",
        "    def _softmax(self, Z):\n",
        "        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
        "\n",
        "    def _cross_entropy_loss(self, Y_pred, Y_true):\n",
        "        m = Y_true.shape[0]\n",
        "        logprobs = -np.log(Y_pred[range(m), Y_true])\n",
        "        loss = np.sum(logprobs) / m\n",
        "        return loss\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        for i in range(self.num_iterations):\n",
        "            # Forward pass\n",
        "            Z = np.dot(X_train, self.W) + self.b\n",
        "            A = self._softmax(Z)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self._cross_entropy_loss(A, y_train)\n",
        "\n",
        "            # Backward pass\n",
        "            dZ = A\n",
        "            dZ[range(X_train.shape[0]), y_train] -= 1\n",
        "            dW = np.dot(X_train.T, dZ) / X_train.shape[0]\n",
        "            db = np.sum(dZ, axis=0, keepdims=True) / X_train.shape[0]\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.W -= self.learning_rate * dW\n",
        "            self.b -= self.learning_rate * db\n",
        "\n",
        "            # Print loss every 100 iterations\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Iteration {i}, Loss: {loss}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        Z = np.dot(X, self.W) + self.b\n",
        "        return np.argmax(self._softmax(Z), axis=1)\n",
        "\n",
        "    def test(self, X_test, y_test):\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(\"Test Accuracy:\", accuracy)\n",
        "        return accuracy\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\n",
        "\n",
        "# Normalize input features\n",
        "X_train /= np.max(X_train)\n",
        "X_test /= np.max(X_test)\n",
        "\n",
        "# Initialize and train the single-layer neural network\n",
        "input_size = X_train.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "model = SingleLayerNeuralNetwork(input_size, num_classes)\n",
        "model.train(X_train, y_train)\n",
        "\n",
        "# Test the model\n",
        "model.test(X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDocuVVaLf1q",
        "outputId": "1aa10a45-e11e-44b8-d110-6c66ef0ee548"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 1.1111745281875733\n",
            "Iteration 100, Loss: 0.810124119302097\n",
            "Iteration 200, Loss: 0.7177803357535996\n",
            "Iteration 300, Loss: 0.6549229901584315\n",
            "Iteration 400, Loss: 0.6092687821219752\n",
            "Iteration 500, Loss: 0.5743399076223297\n",
            "Iteration 600, Loss: 0.5465016924596398\n",
            "Iteration 700, Loss: 0.5235873703687839\n",
            "Iteration 800, Loss: 0.5042346915653483\n",
            "Iteration 900, Loss: 0.48754746381114555\n",
            "Test Accuracy: 0.9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}